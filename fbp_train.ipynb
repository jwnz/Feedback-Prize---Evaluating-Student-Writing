{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QOL Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "logging.set_verbosity_error()\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "class settings:\n",
    "    DATA_PATH   = '../input/feedback-prize-2021/'\n",
    "    WORKERS     = os.cpu_count()\n",
    "    MAX_TOK_LEN = 512\n",
    "    STRIDE      = 256\n",
    "    BATCH       = 4\n",
    "    LR          = [2.5e-05, 2.5e-05, 2.5e-06, 2.5e-07, 2.5e-07]\n",
    "    GRAD_NORM   = 10\n",
    "    EPOCH       = 5\n",
    "    FOLD        = 3\n",
    "\n",
    "    TARGET_ID_MAP = {label:i for i,label in enumerate(output_labels)}\n",
    "    ID_TARGET_MAP = {v:k for k,v in TARGET_ID_MAP.items()}\n",
    "\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    MODEL_BASE_PATH = '../input/model-bin-fbp/'\n",
    "    MODEL_NAME      = 'roberta-base'\n",
    "    MODEL_PATH      = 'model'\n",
    "    WEIGHTS_PATH    = [f'fbp_model_{fold}.pt' for fold in range(5)]\n",
    "\n",
    "\n",
    "label_group = ['O', 'Lead', 'Position', 'Claim', 'Counterclaim', 'Rebuttal', 'Evidence', 'Concluding Statement']\n",
    "label_to_lg_map = {0:0}\n",
    "for target_id, label in settings.ID_TARGET_MAP.items():\n",
    "    if label=='O':\n",
    "        continue\n",
    "    label_to_lg_map[target_id] = label_group.index(label[2:])\n",
    "\n",
    "proba_thresh = {\n",
    "    'Lead': 0.7,\n",
    "    'Position': 0.55,\n",
    "    'Evidence': 0.65,\n",
    "    'Claim': 0.55,\n",
    "    'Concluding Statement': 0.7,\n",
    "    'Counterclaim': 0.5,\n",
    "    'Rebuttal': 0.55,\n",
    "}\n",
    "\n",
    "min_thresh = {\n",
    "    'Lead': 9,\n",
    "    'Position': 5,\n",
    "    'Evidence': 14,\n",
    "    'Claim': 3,\n",
    "    'Concluding Statement': 11,\n",
    "    'Counterclaim': 6,\n",
    "    'Rebuttal': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(id: str, mode: str, split: bool=True):\n",
    "    contents = None\n",
    "\n",
    "    folder = os.path.join(settings.DATA_PATH, mode)\n",
    "    fp = os.path.join(folder, f'{id}.txt')\n",
    "\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    if split:\n",
    "        contents = contents.split()\n",
    "\n",
    "    return contents\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(preprocessed_df, tokenizer, with_labels=True, to_tensor=True):\n",
    "    encoded = tokenizer(\n",
    "        preprocessed_df['split_text'].tolist(),\n",
    "        is_split_into_words=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=settings.STRIDE,\n",
    "        max_length=settings.MAX_TOK_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    if with_labels:\n",
    "        encoded['labels'] = []\n",
    "\n",
    "    encoded['wids'] = []\n",
    "    n = len(encoded['overflow_to_sample_mapping'])\n",
    "    for i in range(n):\n",
    "\n",
    "        # Map back to original row\n",
    "        text_idx = encoded['overflow_to_sample_mapping'][i]\n",
    "        \n",
    "        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n",
    "        word_ids = encoded.word_ids(i)\n",
    "\n",
    "        if with_labels:\n",
    "            # Get word labels of the full un-chunked text\n",
    "            word_labels = preprocessed_df['labels'].iloc[text_idx]\n",
    "\n",
    "            # Get the labels associated with the word indexes\n",
    "            label_ids = [-100 if wid is None else word_labels[wid] for wid in word_ids]\n",
    "            encoded['labels'].append(label_ids)\n",
    "        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n",
    "    \n",
    "    if to_tensor:\n",
    "        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackPrizeDataset(Dataset):\n",
    "    def __init__(self, tokenized_ds):\n",
    "        self.data = tokenized_ds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {k: self.data[k][index] for k in self.data.keys()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(torch.nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        config = AutoConfig.from_pretrained(f'{os.path.join(settings.MODEL_BASE_PATH, settings.MODEL_PATH)}/config.json')\n",
    "\n",
    "        config.update({\n",
    "            'hidden_dropout_prob':0.1,\n",
    "            'layer_norm_eps':1e-7\n",
    "        })\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(f'{os.path.join(settings.MODEL_BASE_PATH, settings.MODEL_PATH)}/pytorch_model.bin', config=config)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.output(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tr_loss = 0\n",
    "\n",
    "    training_iter = tqdm(iter(training_loader))\n",
    "    for n_batch, encoded_input in enumerate(training_iter):\n",
    "        # load data and move to gpu\n",
    "        ids    = encoded_input['input_ids'].to(settings.DEVICE)\n",
    "        mask   = encoded_input['attention_mask'].to(settings.DEVICE)\n",
    "        labels = encoded_input['labels'].to(settings.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # forward pass\n",
    "            pred   = model(ids, mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss   = loss_func(pred.transpose(1,2), labels)\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=settings.GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # propogate backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (n_batch+1) % 200 == 0:    \n",
    "            training_iter.set_description(f'loss: {tr_loss/n_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    '''Aggregate chunked predictions'''\n",
    "    model.eval()\n",
    "\n",
    "    predictions = collections.defaultdict(lambda : collections.defaultdict(lambda: np.zeros(len(settings.ID_TARGET_MAP))))\n",
    "    pred_score = collections.defaultdict(lambda : collections.defaultdict(float))\n",
    "    seen_word_idx = collections.defaultdict(lambda : collections.defaultdict(int))\n",
    "\n",
    "    dataiter = iter(dataloader)\n",
    "    for encoded_input in dataiter:\n",
    "        ids  = encoded_input['input_ids'].to(settings.DEVICE)\n",
    "        mask = encoded_input['attention_mask'].to(settings.DEVICE)\n",
    "\n",
    "        output = model(ids, mask)\n",
    "        output = torch.softmax(output, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "        osm = encoded_input['overflow_to_sample_mapping'].detach().numpy() # doc_id index\n",
    "        word_ids = encoded_input['wids'].detach().numpy()\n",
    "\n",
    "        for i, wids, pred in zip(osm, word_ids, output):\n",
    "            for j, wid in enumerate(wids):\n",
    "                if wid != -1:\n",
    "                    predictions[i][wid] += pred[j]\n",
    "                    seen_word_idx[i][wid] += 1\n",
    "\n",
    "\n",
    "    for i in predictions:\n",
    "        for wid in predictions[i]:\n",
    "            argmax = np.argmax(predictions[i][wid])\n",
    "            pred_score[i][wid] = float(predictions[i][wid][argmax])/seen_word_idx[i][wid]\n",
    "            predictions[i][wid] /= seen_word_idx[i][wid]\n",
    "            predictions[i][wid] = argmax\n",
    "\n",
    "    return predictions, pred_score\n",
    "\n",
    "def inference(model, predictions, pred_score):\n",
    "    model.eval()\n",
    "    result = {'id':[], 'class':[], 'predictionstring':[]}\n",
    "\n",
    "    for idx, i in enumerate(predictions):\n",
    "        \n",
    "        ps = []\n",
    "\n",
    "        last_cls_id = None\n",
    "        for wid, cls_id in predictions[i].items():\n",
    "            cls_id = cls_id if cls_id%2==0 else cls_id+1\n",
    "\n",
    "            if last_cls_id is None or cls_id != last_cls_id:\n",
    "                if len(ps) > 0 and last_cls_id != 0:\n",
    "                    lg = label_group[label_to_lg_map[last_cls_id]]\n",
    "                    if len(ps) >= min_thresh[lg] and np.mean([pred_score[idx][p] for p in ps])>proba_thresh[lg]:\n",
    "                        result['id'].append(i)\n",
    "                        result['class'].append(lg)\n",
    "                        result['predictionstring'].append(' '.join(map(str, ps)))\n",
    "\n",
    "                ps = []\n",
    "                if cls_id == 0:\n",
    "                    last_cls_id = None\n",
    "                    continue\n",
    "\n",
    "            ps.append(wid)\n",
    "            last_cls_id = cls_id\n",
    "\n",
    "        if len(ps) > 0:\n",
    "            if last_cls_id != 0:\n",
    "                if len(ps) >= min_thresh[lg] and np.mean([pred_score[idx][p] for p in ps])>proba_thresh[lg]:\n",
    "                    lg = label_group[label_to_lg_map[last_cls_id]]\n",
    "                    result['id'].append(i)\n",
    "                    result['class'].append(lg)\n",
    "                    result['predictionstring'].append(' '.join(map(str, ps)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validation_ids, validation_dataloader, df_all):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions, pred_score = get_predictions(model, validation_dataloader)\n",
    "\n",
    "    oof = pd.DataFrame(inference(model, predictions, pred_score))\n",
    "    oof['id'] = [validation_ids[_id] for _id in oof.id.tolist()]\n",
    "\n",
    "    f1scores = {}\n",
    "    classes = oof['class'].unique()\n",
    "\n",
    "    for c in classes:\n",
    "        pred_df = oof.loc[oof['class']==c].copy()\n",
    "        gt_df = df_all[df_all.id.isin(validation_ids)].copy()\n",
    "        gt_df = gt_df[gt_df['discourse_type']==c]\n",
    "\n",
    "        f1 = score_feedback_comp(pred_df, gt_df)\n",
    "        f1scores[c] = f1\n",
    "\n",
    "    return f1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_overlap3(set_pred, set_gt):\n",
    "    \"\"\"\n",
    "    Calculates if the overlap between prediction and\n",
    "    ground truth is enough fora potential True positive\n",
    "    \"\"\"\n",
    "    # Length of each and intersection\n",
    "    try:\n",
    "        len_gt = len(set_gt)\n",
    "        len_pred = len(set_pred)\n",
    "        inter = len(set_gt & set_pred)\n",
    "        overlap_1 = inter / len_gt\n",
    "        overlap_2 = inter/ len_pred\n",
    "        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n",
    "    except:  # at least one of the input is NaN\n",
    "        return False\n",
    "\n",
    "def score_feedback_comp_micro3(pred_df, gt_df, discourse_type):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
    "    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
    "    \n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on='id',\n",
    "                           right_on='id',\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    overlaps = [calc_overlap3(*args) for args in zip(joined.predictionstring_pred, \n",
    "                                                     joined.predictionstring_gt)]\n",
    "    \n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    # we don't need to compute the match to compute the score\n",
    "    TP = joined.loc[overlaps]['gt_id'].nunique()\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    TPandFP = len(pred_df)\n",
    "    TPandFN = len(gt_df)\n",
    "    \n",
    "    #calc microf1\n",
    "    my_f1_score = 2*TP / (TPandFP + TPandFN)\n",
    "    return my_f1_score\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    for discourse_type in gt_df.discourse_type.unique():\n",
    "        class_score = score_feedback_comp_micro3(pred_df, gt_df, discourse_type)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'id' : str,\n",
    "    'discourse_type' : str,\n",
    "    'predictionstring' : str,\n",
    "    'kfold' : int\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_NAME, add_prefix_space=True)\n",
    "seed_everything(74)\n",
    "\n",
    "\n",
    "df_all = pd.read_csv('train_folds.csv', dtype=dtype, usecols=list(dtype))\n",
    "preprocessed_df = pd.read_pickle('./id_splittext_labels_df.pkl')\n",
    "\n",
    "# get ids --> filer df --> tokenizer --> build dataset --> init dataloader\n",
    "train_ids = sorted(df_all[~(df_all.kfold==settings.FOLD)].id.unique().tolist())\n",
    "train_df = preprocessed_df[preprocessed_df.id.isin(train_ids)].sort_values(by='id')\n",
    "tokenized_train = tokenize(preprocessed_df[preprocessed_df.id.isin(train_ids)], tokenizer)\n",
    "train_dataset = FeedbackPrizeDataset(tokenized_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=settings.BATCH, pin_memory=True)\n",
    "\n",
    "# get ids --> filer df --> tokenizer --> build dataset --> init dataloader\n",
    "val_ids = sorted(df_all[df_all.kfold==settings.FOLD].id.unique().tolist())\n",
    "val_df = preprocessed_df[preprocessed_df.id.isin(val_ids)].sort_values(by='id')\n",
    "tokenized_val = tokenize(val_df, tokenizer)\n",
    "val_dataset = FeedbackPrizeDataset(tokenized_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=settings.BATCH, pin_memory=True)\n",
    "\n",
    "model = FeedbackModel(len(settings.TARGET_ID_MAP))\n",
    "model.to(settings.DEVICE)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=settings.LR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(settings.EPOCH):\n",
    "    for g in optimizer.param_groups: \n",
    "        g['lr'] = settings.LR[i]\n",
    "\n",
    "    train(model, train_dataloader, optimizer, loss_func)\n",
    "    print('\\n--------END OF EPOCH-------\\n')\n",
    "    v = validation(model, val_ids, val_dataloader, df_all)\n",
    "    print(v)\n",
    "    print('mean', np.mean(list(v.values())))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'fbp_model_{settings.FOLD}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
